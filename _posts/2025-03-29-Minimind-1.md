---
title: 'Minimind解析1：模型结构'
date: 2025-03-29
excerpt: |
  探究Minimind的模型结构（Attention + FFN）
permalink: /posts/2025/03/Minimind解析1：模型结构/
tags:
  - 技术
  - LLM
  - Minimind
---

[Minimind](https://github.com/jingyaogong/minimind)是一个开源的轻量语言模型，标准参数大小为25.8M。在这个系列中，我将记录自己按照官方文档从0开始训练Minimind的过程。

在本篇中，我们首先来关注模型结构。

# 模型结构

![Minimind模型结构图](https://github.com/jingyaogong/minimind/raw/master/images/LLM-structure.png)

模型结构与主流的语言模型结构类似，采用了Transformer的Decoder结构。输入的文字经过Embedding后，经过多层的Transformer Layer，最后经过线性层和softmax输出token的概率分布。

## Attention
Minimind使用的注意力为Grouped Query Attention (GQA)，所以在计算的时候需要单独定义`n_kv_heads`参数，并确保能被`n_heads`整除。

```python
assert args.n_heads % self.n_kv_heads == 0
```

由于使用decoder结构，需要设置`mask`，并使用`resigter_buffer`注册。注意到，`mask`的形状为`(1, 1, args.max_seq_len, args.max_seq_len)`。此处的`args.max_seq_len`是指模型的最大输入长度。

```python
mask = torch.full(
  (1, 1, args.max_seq_len, args.max_seq_len), 
  float("-inf")
)
mask = torch.triu(mask, diagonal=1)
self.register_buffer("mask", mask, persistent=False)
```

### Rotary Position Embedding (RoPE)

Minimind对于投影过后的`query`和`key`，使用了RoPE的位置编码方法。这里与传统的Transformer不同，早期的Transformer实现 (如原始论文中的Transformer) 是直接在输入嵌入上应用位置编码，然后再进行线性变换的。

```python
def apply_rotary_emb(xq, xk, pos_cis):
    def unite_shape(pos_cis, x):
        ndim = x.ndim
        assert 0 <= 1 < ndim
        assert pos_cis.shape == (x.shape[1], x.shape[-1])
        shape = [
          d if i == 1 or i == ndim - 1 else 1 
          for i, d in enumerate(x.shape)
        ]
        return pos_cis.view(*shape)

    xq_ = torch.view_as_complex(
      xq.float().reshape(*xq.shape[:-1], -1, 2)
    )
    xk_ = torch.view_as_complex(
      xk.float().reshape(*xk.shape[:-1], -1, 2)
    )
    pos_cis = unite_shape(pos_cis, xq_)
    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```