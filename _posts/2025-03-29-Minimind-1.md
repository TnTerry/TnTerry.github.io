---
title: 'Minimind解析1：模型结构'
date: 2025-03-29
excerpt: |
  探究Minimind的模型结构（Attention + FFN）
permalink: /posts/2025/03/Minimind解析1：模型结构/
tags:
  - 技术
  - LLM
  - Minimind
---

[Minimind](https://github.com/jingyaogong/minimind)是一个开源的轻量语言模型，标准参数大小为25.8M。在这个系列中，我将记录自己按照官方文档从0开始训练Minimind的过程。

在本篇中，我们首先来关注模型结构。

# 模型结构

![Minimind模型结构图](https://github.com/jingyaogong/minimind/raw/master/images/LLM-structure.png)

模型结构与主流的语言模型结构类似，采用了Transformer的Decoder结构。输入的文字经过Embedding后，经过多层的Transformer Layer，最后经过线性层和softmax输出token的概率分布。

## Attention
Minimind使用的注意力为Grouped Query Attention (GQA)，所以在计算的时候需要单独定义`n_kv_heads`参数，并确保能被`n_heads`整除。

```python
assert args.n_heads % self.n_kv_heads == 0
```

由于使用decoder结构，需要设置`mask`，并使用`resigter_buffer`注册。注意到，`mask`的形状为`(1, 1, args.max_seq_len, args.max_seq_len)`。此处的`args.max_seq_len`是指模型的最大输入长度。

```python
mask = torch.full(
  (1, 1, args.max_seq_len, args.max_seq_len), float("-inf")
)
mask = torch.triu(mask, diagonal=1)
self.register_buffer("mask", mask, persistent=False)
```

### Rotary Position Embedding (RoPE)

Minimind对于投影过后的`query`和`key`，使用了RoPE的位置编码方法。Minimind的方式和LLaMA等大语言模型一致。

```python
def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    """
    预先计算Rope的旋转角度序列
    Args:
        dim: 模型维度，对应d (假定为偶数)
        end: 序列最大长度
        theta: 生成角度序列的底数
    Returns:
        A tensor of shape (end, dim) containing the position encoding.
    """
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # shape: [dim // 2]
    t = torch.arange(end, device=freqs.device)  # type: ignore
    freqs = torch.outer(t, freqs).float()  # type: ignore, shape: [end, dim // 2]

    # 将角度序列 freqs 转换为复数形式的旋转因子，两个参数分别为模长和方向
    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64, shape: [end, dim // 2]
    return pos_cis

def apply_rotary_emb(xq, xk, pos_cis):
  '''
  在query和key上应用RoPE位置编码
  Args:
    xq: query张量，shape: [..., seq_len, dim]
    xk: key张量， shape: [..., seq_len, dim]
    pos_cis: 复数位置编码，shape: [seq_len, dim/2]
             representing e^(iθj) for each position and feature dimension
  Returns:
    Tuple of (xq_out, xk_out) - Query and key tensors with rotary embeddings applied,
    maintaining the same shape and dtype as inputs
  '''
    def unite_shape(pos_cis, x):
      '''
      将pos_cis的shape按照x的统一
      '''
        ndim = x.ndim
        assert 0 <= 1 < ndim
        assert pos_cis.shape == (x.shape[1], x.shape[-1])
        shape = [
          d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)
        ]
        return pos_cis.view(*shape)
    
    # 转为复数域
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))

    pos_cis = unite_shape(pos_cis, xq_)
    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```