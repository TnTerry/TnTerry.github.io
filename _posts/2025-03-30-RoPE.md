---
title: '旋转位置编码 (RoPE)'
date: 2025-03-30
excerpt: |
  RoPE原理与实现
permalink: /posts/2025/03/RoPE/
tags:
  - 技术
  - LLM
  - Embedding
  - RoPE
---

[旋转位置编码 (Rotary Position Embedding, RoPE)](https://arxiv.org/pdf/2104.09864) 是一种用于增强Transformer模型中位置编码的技术。目前，主流的大语言模型大多采用RoPE作为位置编码方法。

# RoPE的原理

## 基础概念

输入序列是由 \\( N \\) 个token组成的序列，其中 \\( w_i \\) 是第 \\( i \\) 个token：

$$
\mathbb{S}_N = \{w_i \}_{i=1}^N \in \mathbb{R}^{N \times d}
$$

我们将输入序列的每个token嵌入到一个 \\( d \\) 维的向量空间中，我们可以得到embedding序列：

$$
\mathbb{E}_N = \{x_i \}_{i=1}^N \in \mathbb{R}^{N \times d}
$$

其中 \\(x_i \\) 是第 \\( i \\) 个token的嵌入向量，且 \\( x_i \in \mathbb{R}^d \\) 是 \\( d \\) 维的向量。

在计算self-attention之前，我们将位置信息与embedding进行融合，并将它们分别转化为 \\( q \\) , \\( k \\) 和 \\( v \\) 向量。**所有位置编码，都是在试图构造一个合适的函数** \\( f(q, k, v) \\)

$$
\begin{aligned}
\boldsymbol{q}_m &= f_q(\boldsymbol{x}_m, m) \\
\boldsymbol{k}_n &= f_k(\boldsymbol{x}_n, n) \\
\boldsymbol{v}_n &= f_v(\boldsymbol{x}_n, n)
\end{aligned}
$$

后续在计算self-attention时，我们会使用 \\( \boldsymbol{q} \\) 和 \\( \boldsymbol{k} \\) 来计算注意力权重，使用 \\( \boldsymbol{v} \\) 来计算输出。

$$
\begin{aligned}
a_{m,n} = \frac{exp(\frac{\boldsymbol{q}_m^\top \boldsymbol{k}_n}{\sqrt{d}})}{\sum_{i=1}^N exp(\frac{\boldsymbol{q}_m^\top \boldsymbol{k}_i}{\sqrt{d}})} \\
\boldsymbol{o}_{m,n} = \sum_{n=1}^N a_{m,n} \boldsymbol{v}_n
\end{aligned}
$$

## 绝对位置编码

对于传统的绝对位置编码，常用的做法是将位置编码 \\( p \\) 直接加到embedding向量上。其中，位置编码 \\( p_i \in \mathbb{R}^d \\) 是一个 \\( d \\) 维的向量，表示第 \\( i \\) 个token的位置信息。

一种经典的计算绝对位置编码的方式为Sinusoidal函数：

$$
\boldsymbol{p}_{i, j} = \begin{cases}
\sin(\frac{i}{10000^{\frac{2j}{d}}}) & \text{if } j = 2k \\
\cos(\frac{i}{10000^{\frac{2j}{d}}}) & \text{if } j = 2k + 1 
\end{cases}
$$

在此基础上，我们对叠加后的embedding向量进行线性变换，得到最终的 \\( \boldsymbol{q} \\) , \\( \boldsymbol{k} \\) 和 \\( \boldsymbol{v} \\) 向量。因此，使用绝对位置编码时，我们构造的函数为：

$$
\begin{aligned}
f_{t:t \in \{q, k, v\}}(\boldsymbol{x}_i, i) &:= W_{t:t \in \{q, k, v\}}(\boldsymbol{x}_i, i)(\boldsymbol{x}_i + \boldsymbol{p}_i) \\
W_{t:t \in \{q, k, v\}} & \in \mathbb{R}^{d \times d}
\end{aligned}
$$

## RoPE

### 复数基本概念

对于复数 \\( z = a + bi \\) ， 我们可以将其看作为一个二维向量 \\( z = (a, b)^ \top \\) ， 其中 \\( a \\) 和 \\( b \\) 分别是实部和虚部。

对于两个复数 \\( z_1 = a_1 + b_1 i \\) 和 \\( z_2 = a_2 + b_2 i \\)，我们可以定义它们的乘法为：

$$
z_1 \cdot z_2 = (a_1 + b_1 i)
\cdot (a_2 + b_2 i) = (a_1 a_2 - b_1 b_2) + (a_1 b_2 + a_2 b_1)i
$$

可以进一步将其写成矩阵和向量相乘的形式，进而将复数的乘法看作向量 \\(z_2 \\)经过矩阵 \\( \begin{pmatrix} a_1 & -b_1 \\ b_1 & a_1 \end{pmatrix} \\) 的旋转变换：

$$
z_1 \cdot z_2 = \begin{pmatrix} a_1 & -b_1 \\ b_1 & a_1 \end{pmatrix} \begin{pmatrix} a_2 \\ b_2 \end{pmatrix}
= \begin{pmatrix} a_1 a_2 - b_1 b_2 \\ a_1 b_2 + a_2 b_1 \end{pmatrix}
$$

复数 \\( z \\) 的模长为 \\( \|z\| = \sqrt{a^2 + b^2} \\) ，其辐角为 \\( \theta = \arctan(\frac{b}{a}) \\) 。因此，上述的矩阵 \\( \begin{pmatrix} a_1 & -b_1 \\ b_1 & a_1 \end{pmatrix} \\) 可以进一步写为：

$$
\begin{align*}
\begin{pmatrix}
a_1 & -b_1 \\
b_1 & a_1
\end{pmatrix}
&= \sqrt{a_1^2 + b_1^2}
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix} \\
&= \| z_1 \| 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix} \\
&= \| z_1 \| \cdot I 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix} \\
&= 
\begin{pmatrix}
\| z_1 \| & 0 \\
0 & \| z_1 \|
\end{pmatrix}
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}.
\end{align*}
$$

因此，旋转矩阵 \\( R \\) 可以表示为：

$$
R = \begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}
$$

---

复数的欧拉公式：

$$
e^{i\theta} = \cos(\theta) + i\sin(\theta)
$$

### RoPE的二维表示

Transformer架构通过self-attention机制捕捉token之间的位置信息。假定query向量 \\( \boldsymbol{q}_m \\) 和key向量 \\( \boldsymbol{k}_n \\) 分别为 \\(\boldsymbol{x} \\) 和 \\(\boldsymbol{x} \\) 变换得到，我们希望query和key之间的内积能够**只通过token embedding以及他们的相对位置**来计算：

$$
\langle f_q(\boldsymbol{x}_m, m), f_k(\boldsymbol{x}_n, n) \rangle  = g(\boldsymbol{x}_m, \boldsymbol{x}_n, m - n)
$$

我们先来看二维平面上的RoPE。假设embedding的维度为 \\( d=2 \\) ，我们可以更方便地利用几何性质进行推导。原论文中对于函数 \\( g \\) 的定义为：

$$
\begin{align*}
f_q(\boldsymbol{x}_m, m) &= (\boldsymbol{W}_q \boldsymbol{x}_m) e^{i m \theta} \\
f_k(\boldsymbol{x}_n, n) &= (\boldsymbol{W}_k \boldsymbol{x}_n) e^{i n \theta} \\
g(\boldsymbol{x}_m, \boldsymbol{x}_n, m - n) &= \text{Re}\left[ (\boldsymbol{W}_q \boldsymbol{x}_m) (\boldsymbol{W}_k \boldsymbol{x}_n)^* e^{i (m - n) \theta} \right]
\end{align*}
$$

其中，\\( \text{Re} \\) 表示取实部，\\( * \\) 表示复共轭； \\( \theta \in \mathbb{R} \\) 是一个超参数，表示旋转的角度。

结合复数的性质以及欧拉公式，我们可以进一步展开函数 \\( g \\) ：

$$
\begin{align*}
g(\boldsymbol{x}_m, \boldsymbol{x}_n, m - n) &= \text{Re}\left[ (\boldsymbol{W}_q \boldsymbol{x}_m) (\boldsymbol{W}_k \boldsymbol{x}_n)^* e^{i (m - n) \theta} \right] \\
&= \left( q_m^{(1)} k_n^{(1)} + q_m^{(2)} k_n^{(2)} \right) \cos\left( (m - n) \theta \right) - \left( q_m^{(2)} k_n^{(1)} - q_m^{(1)} k_n^{(2)} \right) \sin\left( (m - n) \theta \right)
\end{align*}
$$

---

接下来，我们需要证明这个函数 \\( g \\) 是否满足前文定义的内积形式。

对于函数 \\( f_q(\boldsymbol{x}_m, m) \\) 和 \\( f_k(\boldsymbol{x}_n, n) \\)，我们也可以将其展开成为复数形式。首先展开 \\( \boldsymbol{W}_q \boldsymbol{x}_m \\)，并记为 \\( \boldsymbol{q}_m \\) ：

$$
\boldsymbol{q}_m 
= \begin{pmatrix}
\boldsymbol{q}_m^{(1)} \\
\boldsymbol{q}_m^{(2)}
\end{pmatrix}
= \boldsymbol{W}_q \boldsymbol{x}_m 
= \begin{pmatrix}
\boldsymbol{W}_q^{(11)} & \boldsymbol{W}_q^{(12)} \\
\boldsymbol{W}_q^{(21)} & \boldsymbol{W}_q^{(22)}
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{x}_m^{(1)} \\
\boldsymbol{x}_m^{(2)}
\end{pmatrix}
$$

不难发现， \\( \boldsymbol{q}_m \\) 也是一个复数向量。而 \\( e^im \theta = cos(m\theta) + i sin(m\theta) \\)也是复数。因此，两者相乘即为：

$$
\begin{align*}
\boldsymbol{q}_m e^{i m \theta} 
&= \left( \boldsymbol{q}_m^{(1)} + i \boldsymbol{q}_m^{(2)} \right) \left( \cos(m \theta) + i \sin(m \theta) \right) \\
&= \left( \boldsymbol{q}_m^{(1)} \cos(m \theta) - \boldsymbol{q}_m^{(2)} \sin(m \theta) \right) 
+ i \left( \boldsymbol{q}_m^{(2)} \cos(m \theta) + \boldsymbol{q}_m^{(1)} \sin(m \theta) \right)
\end{align*}
$$

最终，\\( f_q(\boldsymbol{x}_m, m) \\)可以展开为：

$$
\begin{align*}
f_q(\boldsymbol{x}_m, m) 
&= (\boldsymbol{W}_q \boldsymbol{x}_m) e^{i m \theta} 
= \boldsymbol{q}_m e^{i m \theta} \\
&= \left[ 
\boldsymbol{q}_m^{(1)} \cos(m \theta) - \boldsymbol{q}_m^{(2)} \sin(m \theta), 
\quad 
\boldsymbol{q}_m^{(2)} \cos(m \theta) + \boldsymbol{q}_m^{(1)} \sin(m \theta) 
\right] \\
&= 
\begin{pmatrix}
\cos(m \theta) & -\sin(m \theta) \\
\sin(m \theta) & \cos(m \theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{q}_m^{(1)} \\
\boldsymbol{q}_m^{(2)}
\end{pmatrix}
\end{align*}
$$

类似的，我们也可以将 \\( f_k(\boldsymbol{x}_n, n) \\) 展开为：

$$
\begin{align*}
f_k(\boldsymbol{x}_n, n) 
&= (\boldsymbol{W}_k \boldsymbol{x}_n) e^{i n \theta} 
= \boldsymbol{k}_n e^{i n \theta} \\
&= \left[ 
\boldsymbol{k}_n^{(1)} \cos(n \theta) - \boldsymbol{k}_n^{(2)} \sin(n \theta), 
\quad 
\boldsymbol{k}_n^{(2)} \cos(n \theta) + \boldsymbol{k}_n^{(1)} \sin(n \theta) 
\right] \\
&= 
\begin{pmatrix}
\cos(n \theta) & -\sin(n \theta) \\
\sin(n \theta) & \cos(n \theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{k}_n^{(1)} \\
\boldsymbol{k}_n^{(2)}
\end{pmatrix}
\end{align*}
$$

因此，\\( f_k(\boldsymbol{x}_n, n) \\)和 \\( f_q(\boldsymbol{x}_m, m) \\) 的内积为：

$$
\begin{align*}
\langle f_q(\boldsymbol{x}_m, m), f_k(\boldsymbol{x}_n, n) \rangle 
&= \left( 
\begin{pmatrix}
\cos(m \theta) & -\sin(m \theta) \\
\sin(m \theta) & \cos(m \theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{q}_m^{(1)} \\
\boldsymbol{q}_m^{(2)}
\end{pmatrix}
\right)^\top
\left(
\begin{pmatrix}
\cos(n \theta) & -\sin(n \theta) \\
\sin(n \theta) & \cos(n \theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{k}_n^{(1)} \\
\boldsymbol{k}_n^{(2)}
\end{pmatrix}
\right) \\
&= 
\begin{pmatrix}
\boldsymbol{q}_m^{(1)} & \boldsymbol{q}_m^{(2)}
\end{pmatrix}
\begin{pmatrix}
\cos(m \theta) & \sin(m \theta) \\
-\sin(m \theta) & \cos(m \theta)
\end{pmatrix}
\begin{pmatrix}
\cos(n \theta) & -\sin(n \theta) \\
\sin(n \theta) & \cos(n \theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{k}_n^{(1)} \\
\boldsymbol{k}_n^{(2)}
\end{pmatrix} \\
&= 
\begin{pmatrix}
\boldsymbol{q}_m^{(1)} & \boldsymbol{q}_m^{(2)}
\end{pmatrix}
\begin{pmatrix}
\cos((m - n)\theta) & -\sin((m - n)\theta) \\
\sin((m - n)\theta) & \cos((m - n)\theta)
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{k}_n^{(1)} \\
\boldsymbol{k}_n^{(2)}
\end{pmatrix} \\
&= \left( q_m^{(1)} k_n^{(1)} + q_m^{(2)} k_n^{(2)} \right) \cos\left( (m - n) \theta \right) - \left( q_m^{(2)} k_n^{(1)} - q_m^{(1)} k_n^{(2)} \right) \sin\left( (m - n) \theta \right)
\end{align*}
$$

最终的形式符合我们之前定义的函数 \\( g(\boldsymbol{x}_m, \boldsymbol{x}_n, m - n) \\) 的形式。

### RoPE的一般形式

原文中将维度结果推广到任意维度的方法很简单：保持\\( f_k(\boldsymbol{x}_n, n) \\)和 \\( f_q(\boldsymbol{x}_m, m) \\) 的基本形式不变，而是将 \\( d \\) 维空间拆分为 \\( d/2 \\) 个二维空间。

$$
f_{\{q,k\}}(\boldsymbol{x}_m, m) = \boldsymbol{R}_{\Theta, M}^d \boldsymbol{W}_{\{q,k\}} \boldsymbol{x}_m
$$

其中，\\( \boldsymbol{R}_{\Theta, M}^d \\) 是一个 \\( d \\times d \\) 的旋转矩阵，且由 \\( d/2 \\) 个 \\( 2 \times 2 \\) 的旋转矩阵组成。

$$
\boldsymbol{R}_{\Theta, m}^d = 
\begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots & 0 & 0 \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \\
0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2} & \cos m\theta_{d/2}
\end{pmatrix}
$$

矩阵中的 \\( \theta_i \\) 表示第 \\( i \\) 个二维空间的旋转角度，是预先定义的超参数。且满足：

$$
\Theta = \left\{ \theta_i = 10000^{-2(i-1)/d}, \; i \in [1, 2, \ldots, d/2] \right\}
$$

此时，我们将这个函数进一步代入self-attention的计算中，可以得到：

$$
\begin{align*}
\boldsymbol{q}_m^\top \boldsymbol{k}_n &= (\boldsymbol{R}_{\Theta, m}^d \boldsymbol{W}_{q} \boldsymbol{x}_m)^\top (\boldsymbol{R}_{\Theta, m}^d \boldsymbol{W}_{q} \boldsymbol{x}_m) \\
&= x_m^\top \boldsymbol{W}_{q}^\top (\boldsymbol{R}_{\Theta, m}^d)^\top \boldsymbol{R}_{\Theta, n}^d \boldsymbol{W}_{k} \boldsymbol{x}_n \\
&= x_m^\top \boldsymbol{W}_{q}^\top \boldsymbol{R}_{\Theta, m-n}^d \boldsymbol{W}_{k} \boldsymbol{x}_n \\
\end{align*}
$$

其中

$$
\boldsymbol{R}_{\Theta, m-n}^d = (\boldsymbol{R}_{\Theta, m}^d)^\top \boldsymbol{R}_{\Theta, n}^d
$$

注意到，这里的旋转矩阵 \\( \boldsymbol{R}_{\Theta, m-n}^d \\) 是正交矩阵，也就是说其不会在编码位置信息的时候影响数值的稳定性。

### 高效实现

注意到，原始的旋转矩阵 \\( \boldsymbol{R}_{\Theta}^d \\) 是稀疏的，直接使用矩阵乘法会浪费计算资源。可以使用更加高效的方法进行计算：

$$
\mathbf{R}_{\Theta, m}^d \boldsymbol{x} = 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\vdots \\
x_{d-1} \\
x_d
\end{pmatrix} \otimes 
\begin{pmatrix}
\cos m\theta_1 \\
\cos m\theta_2 \\
\vdots \\
\cos m\theta_{d/2}
\end{pmatrix} + 
\begin{pmatrix}
-x_2 \\
x_1 \\
-x_4 \\
x_3 \\
\vdots \\
-x_d \\
x_{d-1}
\end{pmatrix} \otimes 
\begin{pmatrix}
\sin m\theta_1\\
\sin m\theta_2 \\
\vdots \\
\sin m\theta_{d/2}
\end{pmatrix}
$$

# RoPE的实现

在LLaMA等模型中，RoPE的实现是通过将输入序列的embedding向量转化为复数形式，然后在复数域上进行旋转操作。

```python

# 生成旋转矩阵
def precompute_freqs_cis(
    dim: int, 
    seq_len: int, 
    theta: float = 10000.0
):
    """
    预计算旋转位置编码的复数形式
    Args:
        dim: 模型维度，对应d (假定为偶数)
        seq_len: 序列最大长度
        theta: 生成角度序列的底数
    Returns:
        A tensor of shape (seq_len, dim) containing the position encoding.
    """
    # 对元素两两分组，每组的旋转角度为theta_i（theta的整数倍）
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # shape: [dim // 2]
    # token序列索引
    t = torch.arange(seq_len, device=freqs.device)
    freqs = torch.outer(t, freqs).float() # shape = [seq_len, dim // 2] 

    # 将角度序列 freqs 转换为复数形式的旋转因子，两个参数分别为模长和方向
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs) 
    return freqs_cis

def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    在query和key上应用RoPE位置编码
    Args:
        xq: query张量，shape: [..., seq_len, dim]
        xk: key张量， shape: [..., seq_len, dim]
        freqs_cis: 复数位置编码，shape: [seq_len, dim/2]
                 representing e^(iθj) for each position and feature dimension
    Returns:
        Tuple of (xq_out, xk_out) - Query and key tensors with rotary embeddings applied,
        maintaining the same shape and dtype as inputs
    """

    # xq.shape: [batch_size, seq_len, dim]
    # xq_.shape: [batch_size, seq_len, dim // 2, 2]
    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)
    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)
    
    # 转为复数域
    xq_ = torch.view_as_complex(xq_)
    xk_ = torch.view_as_complex(xk_)
    
    # 应用旋转操作，然后将结果转回实数域
    # xq_out.shape = [batch_size, seq_len, dim]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

```

在定义好了旋转位置编码的函数之后，我们可以在Attention模型的forward函数中使用它。在attention操作之前，对投影过后的`query`和`key`应用旋转位置编码。

```python
class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.wq = Linear(...)
        self.wk = Linear(...)
        self.wv = Linear(...)
        
        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)

    def forward(self, x: torch.Tensor):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(batch_size, seq_len, dim)
        xk = xk.view(batch_size, seq_len, dim)
        xv = xv.view(batch_size, seq_len, dim)

        # attention 操作之前，应用旋转位置编码
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # 计算注意力权重
        scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim) # [batch, seq_len, seq_len]
        scores = F.softmax(scores.float(), dim=-1)
        output = torch.matmul(scores, xv)  # [batch, seq_len, dim]
        return output
```