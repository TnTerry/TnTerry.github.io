---
title: '旋转位置编码 (RoPE)'
date: 2025-03-30
excerpt: |
  RoPE原理与实现
permalink: /posts/2025/03/RoPE/
tags:
  - 技术
  - LLM
  - Embedding
  - RoPE
---

[旋转位置编码 (Rotary Position Embedding, RoPE)](https://arxiv.org/pdf/2104.09864) 是一种用于增强Transformer模型中位置编码的技术。目前，主流的大语言模型大多采用RoPE作为位置编码方法。

# RoPE的原理

## 基础概念

输入序列是由 \\( N \\) 个token组成的序列，其中 \\( w_i \\) 是第 \\( i \\) 个token：

$$
\mathbb{S}_N = \{w_i \}_{i=1}^N \in \mathbb{R}^{N \times d}
$$

我们将输入序列的每个token嵌入到一个 \\( d \\) 维的向量空间中，我们可以得到embedding序列：

$$
\mathbb{E}_N = \{x_i \}_{i=1}^N \in \mathbb{R}^{N \times d}
$$

其中 \\(x_i \\) 是第 \\( i \\) 个token的嵌入向量，且 \\( x_i \in \mathbb{R}^d \\) 是 \\( d \\) 维的向量。

在计算self-attention之前，我们将位置信息与embedding进行融合，并将它们分别转化为 \\( q \\) , \\( k \\) 和 \\( v \\) 向量。**所有位置编码，都是在试图构造一个合适的函数** \\( f(q, k, v) \\)

$$
\begin{aligned}
q_m &= f_q(x_m, m) \\
k_n &= f_k(x_n, m) \\
v_n &= f_v(x_n, m)
\end{aligned}
$$

后续在计算self-attention时，我们会使用 \\( q \\) 和 \\( k \\) 来计算注意力权重，使用 \\( v \\) 来计算输出。

$$
\begin{aligned}
a_{m,n} = \frac{exp(\frac{q_m^\top k_n}{\sqrt{d}})}{\sum_{i=1}^N exp(\frac{q_m^\top k_i}{\sqrt{d}})} \\
o_{m,n} = \sum_{n=1}^N a_{m,n} v_n
\end{aligned}
$$

## 绝对位置编码

对于传统的绝对位置编码，常用的做法是将位置编码 \\( p \\) 直接加到embedding向量上。其中，位置编码 \\( p_i \in \mathbb{R}^d \\) 是一个 \\( d \\) 维的向量，表示第 \\( i \\) 个token的位置信息。

一种经典的计算绝对位置编码的方式为Sinusoidal函数：
$$
p_{i, j} = \begin{cases}
\sin(\frac{i}{10000^{\frac{2j}{d}}}) & \text{if } j = 2k \\
\cos(\frac{i}{10000^{\frac{2j}{d}}}) & \text{if } j = 2k + 1 
\end{cases}
$$

在此基础上，我们对叠加后的embedding向量进行线性变换，得到最终的 \\( q \\) , \\( k \\) 和 \\( v \\) 向量。因此，使用绝对位置编码时，我们构造的函数为：

$$
\begin{aligned}
f_{t:t \in \{q, k, v\}}(x_i, i) \coloneqq W_{t:t \in \{q, k, v\}}(x_i, i)(x_i + p_i) \\
W_{t:t \in \{q, k, v\}} \coloneqq \mathbb{R}^{d \times d}
\end{aligned}
$$