---
title: '混合专家 (MoE)'
date: 2025-05-07
excerpt: |
  MoE原理与实现
permalink: /posts/2025/03/MoE/
tags:
  - 技术
  - LLM
  - MoE
---

混合专家 (Mixture of Experts, MoE) 是一种神经网络架构。MoE的核心思想在于**整合多个模型或者专家**，进而提升整体的模型性能。

# MoE的结构

## 简单MoE

回顾Multi-head Attention的机制，我们可以先设计一个简单的MoE网络：即每个Expert专家都是一个简单的FFN前馈网络。结构如下图[^1]所示：

![简单MoE结构](https://bruceyuan.com/llms-zero-to-hero/basic-moe-model.png)

[^1]:https://bruceyuan.com/llms-zero-to-hero/the-way-of-moe-model-evolution.html